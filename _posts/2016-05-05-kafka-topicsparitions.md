---
layout: post
title: "在Kafka集群内, 如何权衡Topics／Paritions数量"
keywords: ["kafka","partition"]
description: "How to Determine Kafka Topic/Partition Number"
category: "apache-kafka"
tags: ["apache-kafka"]
---
{% include JB/setup %}

#在Kafka集群内, 如何权衡Topic/Partition们的数量												原文发布http://www.confluent.io/blog/how-to-choose-the-number-of-topicspartitions-in-a-kafka-cluster/作者 Jun Rao								03/12, 2015译者 BrianLing这是很多Kafka用户都会提及的常见问题。本文旨在解释几个决定性的重要因素 以及提供一些简单的公式计算。##更多的分区会提供更大吞吐量 首先需要理解的是在Kafka内，单主题的分区是最小的并发单元。在消息生产和代理两方，写入不同的分区可以完全并发执行。因而，一些代价昂贵的操作（例如：消息压缩）可以利用更多硬件资源来并发执行。对消息消费方而言，Kafka通常将单个分区的数据和单个消费线程配对。因此，消费的并行度（在一个消费组群内）受限于分区的数量。一般来说，越多的Kafka 分区数量，就能达到更大的吞吐量。
粗略的计算分区数量公式是基于吞吐量来推导的。你可以先测试在单个分区能达到最大的发送吞吐量(简称: p)和消费吞吐量(简称: c)。假设你目标吞吐量是t，那么意味着你最少需要分区数量是t/p和t/c的最大值。生产者能达到的单Partition最大吞吐量取决于相关的配置，例如批处理大小，压缩策略，消息确认方式，副本数量等。尽管如此，通常单生产者能在单分区 达到10+MB/s 推送速率（具体参考基准测试）。消费者的吞吐量通常取决于应用程序，因为它直接关系到消费者的逻辑， 它是多快来处理单个消息。总而言之，你真的很有必要去评测在实际环境下的生产和消费吞吐量。
虽然，随着时间的推移，增加分区数量都是可行的，但是如果发送的消息关联指定的键值，有一点需要特别注意。当发送带键值的消息时，Kafka将根据键的哈希值确定地将消息映射到特定的分区。Kafka保证同一键值的消息总是会路由到同一分区。 这个特性的保障对特定应用非常重要，因为同一分区的消息总是按序交付給消费者，如果分区数量改变，类似的保障就不再生效的。为避免上述情况，通常的做法是过度分配多一些的分区。大致说来，你可以根据未来（例如一两年后）可期待的吞吐量来决定分区数量。最初，你可以基于当前吞吐量构建小规模的Kafka集群。随着时间的推移，你可以增加更多的消息代理节点并适当地迁移部分分区到新代理节点（类似操作可以在运行期完成）。通过这种方式， 你可以在不破坏应用程序行为（在消息键值被利用的场景下）的同时，继续根据吞吐量的增长同步扩展。
在选择分区数量的时候，除了吞吐量，还有其他因素也值得考量。如你所见，过多的分区也可能引发负面影响。##更多的分区需要打开更多的文件描述符在消息代理节点中，每个分区都会映射文件系统中的一个目录。通常每个消息段会涉及两个文件（一个是为了存储索引，另一个是为了保存实际数据）。目前，每个Kafka服务节点会为每个消息段的索引和数据文件打开单个文件句柄。因此，更多的分区，更受限彧底层操作系统，它需要配置允许打开更多的文件句柄。这通常是配置问题。我们曾在产品Kafka集群上，每个服务器运行3万多个文件句柄。
###过多分区可能会影响可用性
	Kafka支持跨集群间的复制，以提供更高的系统可用性和数据持久性。单个分区可以拥有多个副本，每个副本可以存放在不同的代理节点上。其中一个副本被指定为领导，其余的副本都是相应的追随者。Kafka在内部自动管理所有这些副本，并确保他们都保持同步。副本的领导将为该分区的生产和消费者提供服务。当一个代理节点发生故障时，相关主管的分区将暂时不可用。Kafka会自动将这些不可用分区的领导迁移到其他副本的节点上，以继续为客户提供服务。这个过程是由一个被指定为控制器的Kafka代理节点完成。这涉及到为每个相关分区读写Zookeeper的元数据。目前。控制节点对Zookeeper的操作是串行完成的。
	
	一般情况下，当一个代理节点优雅地关闭时，则控制节点讲主动地一次性把所有分区领导们移出即将关闭的节点。每次单个分区领导迁移都需要几毫米的耗时。因而，从客户端角度来看，当膜代理节点关闭时，只有很小的不可用时间窗口。
	
	然而，当代理节点强制关闭（例如，kill -9）时, 可感知的不可用性将和分区数量成正比。假定一个代理节点有2000个分区，每个分区有2个副本。粗略地说，这是1000个分区的领导者。当这个代理节点被非彻底关闭时，这些1000分区将在同一时间不可用。假设每个分区花费5毫秒选举出新领导。那他为这1000分区总共耗时5秒来推选新领导。所以，对于一些分区，他们可感知的不可用时间将是5秒加上检测到该故障的时间。
		如果很不幸，故障代理节点可能是控制节点。这种情况下，直到控制器故障切换到新的代理节点，才会启动选举新的分区领导。控制器的故障切换会自动发生，单需要新控制器在初始化阶段从Zookeeper读取每个分区的元数据。例如，如果Kafka集群有10,000分区，每个分区从Zookeeper初始化元数据需要2毫秒耗时，那累计是20多秒的不可用时间窗口。
		总体而言，强制关闭的故障比较少见。然而，如果你对这些罕见情况引发的不可用性敏感，最好把每个代理节点的分区数量限制在2-4千之内，而且集群总体分区数目也要低于几万。##更多分区会增加端到端延迟Kafka端到端的延迟被定义为从生产者时发布的消到消费者读取消息的耗时。对每个分区而言，在消息被彻底提交之后（例如：消息已经复制到所有相关同步副本节点），Kafka只会暴露单个消息給消费者。因此，提交单个消息的耗时将很大程度上影响端到端的延时。默认，每个Kafka代理节点只会使用单线程从另一个代理节点复制所有分区副本共享的数据。我们的经验表明跨代理节点复制1000分区会增加额外20毫秒的延迟，这也意味着端到端的延迟至少20毫秒。对于实时应用而言，这样的延迟过高了。
需要注意的是,类似问题会在大集群场景下得以缓解。例如,假设在单个代理节点上有1000个分区领导，同时在同一个Kafka集群内有其他10个代理节点。平均下来，这10个节点各自只需要从第一个代理获取100个分区的数据。因此,由提交消息而额外增加的延迟将仅仅是几毫秒，而不是几十毫秒。
根据经验，如果你对延迟敏感，将每个代理节点的分区数量限制在100*b*r 可能是个不错的主意,这里的b是Kafka集群内代理节点的数量，r是副本因子。
##更多分区会要求客户端更多内存分配在最近0.8.2发布和我们相应推出的Confluent平台1.0中，我们研发了更高效的Java生产者。新生产者其中一个不错的功能是允许用户设置用于缓冲传入消息的内存量的上限。在内部，生产者会为每个分区缓存消息。经过足够的数据积累或达到足够的时间间隔，积累的消息被从缓冲区删除，并发送到代理。一旦增加分区数目，则消息会在生产者的更多分区缓冲区中累积。所需的内存总量可能会超过设定的内存限制。当这种情况发生时，生产者必须阻止或删除任何新消息，但这两个方案都不够理想。为了防止类似情况发生，你将需要为生产者重新配置具有较大的内存空间。
根据经验，为了实现较好的吞吐量，我们应该为生产者的每个分区预留几十KB的空间，并调整总体内存大小，以防分区数量猛增。
类似的问题也会发生在消费者端。消费者会从每个分区批量获取消息。更多的分区消费者读取，它就需要更多的内存空间。然而，这通常是非实时消费者才能遇到的问题。##总结总的来说，在Kafka集群中，更多的集群分区会导致高吞吐量。然而，我们也必须清楚地意识到过多分区对单节点和整个集群引发的潜在影响。例如可用性和延迟性。未来，我们计划改善这些限制，使得在Kafka高分区数目下更可扩展。